import{s as ze,n as X}from"../chunks/scheduler.ed1f4acb.js";import{S as De,i as Ee,r as J,s as u,g as d,u as V,c as m,h,j as Re,x as f,f as g,k as s,v as Z,a as b,y as o,d as U,t as Y,w as K}from"../chunks/index.bf659680.js";import{C as Be}from"../chunks/common-header.8ff1db57.js";import{H as We}from"../chunks/home-hero-icon.62369fab.js";import{S as ve}from"../chunks/shadow-box.749b2d96.js";import{b as S}from"../chunks/paths.7359f96c.js";import{m as Oe}from"../chunks/made-to-measure.fed32c4f.js";function Ge(A){let n,x;return n=new We({props:{slot:"icon"}}),{c(){J(n.$$.fragment)},l(e){V(n.$$.fragment,e)},m(e,i){Z(n,e,i),x=!0},p:X,i(e){x||(U(n.$$.fragment,e),x=!0)},o(e){Y(n.$$.fragment,e),x=!1},d(e){K(n,e)}}}function Ne(A){let n,x="What do we mean by “artificial intelligence”?",e,i,C=`Artificial intelligence refers to systems that resemble, carry out, or mimic functions that
      are typically thought of as requiring human-like intelligence. Examples of these systems
      include facial recognition software, natural language processing, and other applications. The
      development and deployment of these technologies can have a significant impact on human
      rights, including the rights to freedom of expression, access to information, to form and hold
      opinions, non-discrimination, and privacy. In this Resource Hub, we use “artificial
      intelligence” as a blanket term to encompass forms of automation, algorithmic decision making,
      and machine learning, although these technologies have different capacities and functions.`,w,r,k=`Generative AI refers to AI models and tools trained on multi-format datasets with the
      potential to generate human-like outputs such as text, image, video, music or action based on
      the patterns identified in the data it was trained on (including both novel output and
      modified existing content).`,y,c,_=`See the <a href="${`${S}/glossary`}">Glossary</a> to read more about these terms.`;return{c(){n=d("h3"),n.textContent=x,e=u(),i=d("p"),i.textContent=C,w=u(),r=d("p"),r.textContent=k,y=u(),c=d("p"),c.innerHTML=_,this.h()},l(a){n=h(a,"H3",{class:!0,"data-svelte-h":!0}),f(n)!=="svelte-1y2w7yi"&&(n.textContent=x),e=m(a),i=h(a,"P",{class:!0,"data-svelte-h":!0}),f(i)!=="svelte-13j70j8"&&(i.textContent=C),w=m(a),r=h(a,"P",{class:!0,"data-svelte-h":!0}),f(r)!=="svelte-1g26fvq"&&(r.textContent=k),y=m(a),c=h(a,"P",{"data-svelte-h":!0}),f(c)!=="svelte-fe8jga"&&(c.innerHTML=_),this.h()},h(){s(n,"class","font-extrabold text-2xl leading-8 mb-12"),s(i,"class","mb-4"),s(r,"class","mb-4")},m(a,l){b(a,n,l),b(a,e,l),b(a,i,l),b(a,w,l),b(a,r,l),b(a,y,l),b(a,c,l)},p:X,d(a){a&&(g(n),g(e),g(i),g(w),g(r),g(y),g(c))}}}function Je(A){let n,x="How much do private platforms know about you?",e,i,C=`A team of documentary filmmakers set out to answer this question – and to illustrate how the
      digital traces of everything we do online can be used to assemble powerful profiles about
      individuals, and even to manipulate our opinions and expressions. Made to Measure exposes the
      power of surveillance capitalism and how much private companies know – and can predict – about
      who we are.`,w,r,k=`<img class="mb-4 w-full object-cover rounded-lg transition-transform duration-200 hover:scale-105" src="${Oe}" alt="Made to Measure"/>`,y,c,_=`This interactive film was co-produced in 2022 by the Organization for Security and
      Co-operation in Europe, Office of the Representative on Freedom of the Media (OSCE RFoM), as
      part of the “Spotlight on Artificial Intelligence & Freedom of Expression (SAIFE)” project.`,a,l,N='<span>Watch</span> <span class="rounded-full border-2 border-blue p-2 shrink-0"><svg class="w-6 h-6 stroke-blue-osce transition-transform duration-300 group-hover:translate-x-1" viewBox="0 0 9 15" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M1.29932 1.06445L7.29932 7.06445L1.29932 13.0645" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"></path></svg></span>';return{c(){n=d("h3"),n.textContent=x,e=u(),i=d("p"),i.textContent=C,w=u(),r=d("a"),r.innerHTML=k,y=u(),c=d("p"),c.textContent=_,a=u(),l=d("a"),l.innerHTML=N,this.h()},l(p){n=h(p,"H3",{class:!0,"data-svelte-h":!0}),f(n)!=="svelte-rs1m01"&&(n.textContent=x),e=m(p),i=h(p,"P",{class:!0,"data-svelte-h":!0}),f(i)!=="svelte-41d225"&&(i.textContent=C),w=m(p),r=h(p,"A",{href:!0,rel:!0,"data-svelte-h":!0}),f(r)!=="svelte-nzmhnf"&&(r.innerHTML=k),y=m(p),c=h(p,"P",{class:!0,"data-svelte-h":!0}),f(c)!=="svelte-1ohubly"&&(c.textContent=_),a=m(p),l=h(p,"A",{class:!0,href:!0,rel:!0,"data-svelte-h":!0}),f(l)!=="svelte-cl35ei"&&(l.innerHTML=N),this.h()},h(){s(n,"class","font-extrabold text-2xl leading-8 mb-12"),s(i,"class","mb-4"),s(r,"href","https://www.madetomeasure.online/en/"),s(r,"rel","noopener noreferrer"),s(c,"class","mb-4"),s(l,"class","group flex items-center space-x-2 hover:border-none ml-auto"),s(l,"href","https://madetomeasure.online/en"),s(l,"rel","noopener noreferrer")},m(p,$){b(p,n,$),b(p,e,$),b(p,i,$),b(p,w,$),b(p,r,$),b(p,y,$),b(p,c,$),b(p,a,$),b(p,l,$)},p:X,d(p){p&&(g(n),g(e),g(i),g(w),g(r),g(y),g(c),g(a),g(l))}}}function Ve(A){let n,x="Safeguarding freedom of expression in the age of AI: guidelines for policy makers",e,i,C=`<p class="w-10/12">Read the <a href="${`${S}/dos-and-donts`}">Dos and Don’ts</a> summarizing key points that policy
        makers should take into consideration as they develop AI strategies and policies.</p> <a href="${`${S}/dos-and-donts`}" class="group flex items-center hover:border-none"><span class="rounded-full border-2 border-blue p-2 shrink-0"><svg class="w-6 h-6 stroke-blue-osce transition-transform duration-300 group-hover:translate-x-1" viewBox="0 0 9 15" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M1.29932 1.06445L7.29932 7.06445L1.29932 13.0645" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"></path></svg></span></a>`,w,r,k=`<p class="w-10/12">Go to <a href="${`${S}/resources`}">Resources</a> to explore key readings, research, and materials
        on how AI systems are being developed and deployed and the impact this has on freedom of expression
        and media freedom.</p> <a href="${`${S}/resources`}" class="group flex items-center hover:border-none"><span class="rounded-full border-2 border-blue p-2 shrink-0"><svg class="w-6 h-6 stroke-blue-osce transition-transform duration-300 group-hover:translate-x-1" viewBox="0 0 9 15" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M1.29932 1.06445L7.29932 7.06445L1.29932 13.0645" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"></path></svg></span></a>`,y,c,_=`<p class="w-10/12">Read the <a href="${`${S}/policy-2025`}">Policy Manual to Safeguards Media Freedom in the Age of Big Tech Platforms and AI (2025)</a>
        and the <a href="${`${S}/policy`}">SAIFE Policy Manual (2022)</a> for how to develop human-rights
        centered AI policies that protect freedom of expression and media pluralism.</p> <a href="${`${S}/policy`}" class="group flex items-center hover:border-none"><span class="rounded-full border-2 border-blue p-2 shrink-0"><svg class="w-6 h-6 stroke-blue-osce transition-transform duration-300 group-hover:translate-x-1" viewBox="0 0 9 15" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M1.29932 1.06445L7.29932 7.06445L1.29932 13.0645" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"></path></svg></span></a>`;return{c(){n=d("h3"),n.textContent=x,e=u(),i=d("div"),i.innerHTML=C,w=u(),r=d("div"),r.innerHTML=k,y=u(),c=d("div"),c.innerHTML=_,this.h()},l(a){n=h(a,"H3",{class:!0,"data-svelte-h":!0}),f(n)!=="svelte-18vrmhk"&&(n.textContent=x),e=m(a),i=h(a,"DIV",{class:!0,"data-svelte-h":!0}),f(i)!=="svelte-19clvj1"&&(i.innerHTML=C),w=m(a),r=h(a,"DIV",{class:!0,"data-svelte-h":!0}),f(r)!=="svelte-bdbdm1"&&(r.innerHTML=k),y=m(a),c=h(a,"DIV",{class:!0,"data-svelte-h":!0}),f(c)!=="svelte-9wksf8"&&(c.innerHTML=_),this.h()},h(){s(n,"class","font-extrabold text-2xl leading-8 mb-12"),s(i,"class","group flex flex-row items-center space-x-2 mb-4"),s(r,"class","group flex flex-row items-center space-x-2 mb-4"),s(c,"class","group flex flex-row items-center space-x-2 mb-4")},m(a,l){b(a,n,l),b(a,e,l),b(a,i,l),b(a,w,l),b(a,r,l),b(a,y,l),b(a,c,l)},p:X,d(a){a&&(g(n),g(e),g(i),g(w),g(r),g(y),g(c))}}}function Ze(A){let n,x,e,i,C=`In 1842, Ada Lovelace had declared that “the Analytical Engine has no pretensions whatever to
    originate anything. It can do whatever we know how to order it to perform. It can follow
    analysis, but it has no power of anticipating any analytical relations or truths. Its province
    is to assist us in making available what we are already acquainted with.”`,w,r,k=`Almost two centuries later, we are grappling with these same fundamental issues, as the impacts
    of artificial intelligence on society unfold. Technological advances in the past decade have
    fueled a boom in AI applications—in communications, banking, health care, manufacturing,
    criminal justice, and national security—in ways that are radically reshaping our private and
    public lives. The advent of generative AI applications, in particular, accelerated many of these
    developments.`,y,c,_=`While proponents say these technologies hold the promise of tackling some of society’s most
    complex challenges, a growing number of policy experts and advocates warn that AI has as much
    potential to erode human rights, harm our democracies, and further entrench and amplify
    discrimination and global inequity at scale. And in the rush to innovate, it has become
    increasingly apparent that neither governments nor private companies have put adequate human
    rights protections and governance mechanisms in place to ensure that AI’s benefits to society
    outweigh the harms.`,a,l,N,p,$="How does AI shape our information environment?",ee,j,be=`A handful of dominant digital platforms and AI companies increasingly shape how information is
    produced, distributed, accessed, and monetized. Internet intermediaries, and social media
    platforms in particular, have increasingly turned to AI systems to help personalize and
    disseminate news and information, as well as to flag and remove harmful or unlawful content.
    According to the annual Digital News Report by the Reuters Institute of Journalism at Oxford,
    the vast majority of news is accessed through platforms – search engines, social media, news
    aggregators, and, increasingly chatbots.`,te,F,xe=`As AI systems play a central role in defining our news and information diets, we must ask: how
    are these systems influencing—or threatening—our rights to freedom of expression and access to
    information? What are the broader implications for the free exchange of ideas and opinions and
    for our ability to exercise the political rights and freedoms that foster healthy democratic
    societies?`,ne,q,we=`AI technologies and systems to disseminate content are designed to deliver content based on how
    much engagement it is likely to generate—as measured by the number of views, likes, and
    shares—rather than for its accuracy, diversity, or public-interest value. Platforms target
    individuals and groups with tailored content (and ads) based on algorithmically-generated
    collections of our data, touching everything from our political affiliations and preferences, to
    our gender, race, age, income, and other characteristics.`,ae,z,ye=`Moreover, the use of AI to moderate content can lead to problematic results: these systems can
    misidentify and remove legitimate content in some instances—a form of prior restraint that
    results in over removal and thus censorship—while failing to accurately or consistently spot
    harmful, abusive, or illegal content, in which case these materials continue to proliferate
    online. Both false positives and false negatives can have severe impacts on the online
    information landscape and enjoyment of freedom of expression of individuals and communities. The
    removal of legitimate speech by automated tools can restrict access to information, limit media
    pluralism, and threaten freedom of expression. Investigations by media and civil society have
    revealed numerous cases in which social media platforms have blocked or removed opposition posts
    and hashtags during times of political unrest. At the other end of the spectrum, the flood of
    hateful, racist, toxic speech can have a deeply corrosive effect on public discourse but can
    also perpetuate discrimination and other types of human rights harms toward individuals and
    groups both on- and offline.`,oe,D,$e=`Ample case studies<a class="fgoto" id="b1" href="#f1">[1]</a> point to how AI-based content
    curation and moderation can distort or reshape online news diets in ways that restrict or limit
    the free flow of ideas and opinions—essential prerequisites of healthy democratic systems.
    Research has shown how AI-based content curation can create so-called “filter bubbles” or “echo
    chambers,” which dramatically limit exposure to views and information that do not align with an
    individual’s already-established beliefs. While the effects of such “filter bubbles” on an
    individuals’ beliefs or behaviors is unclear—and in some cases their existence has been found to
    have been overstated—what is certain is that filter bubbles can accentuate and drive political
    and societal polarization, which in turn can have a deleterious and harmful effect on the open
    exchange of opinions and ideas.<a class="fgoto" id="b2" href="#f2">[2]</a>`,se,E,Ce=`Generative AI applications perpetuate and accelerate many of the challenges to free expression
    and media freedom already present in the platform environment. They risk further concentrating
    power, disrupting news production and revenue models, and relying on large volumes of
    journalistic and creative content for training that is often used without meaningful consent,
    compensation, or attribution. At the same time, generative AI existing challenges through
    inaccuracies and so-called ‘hallucinations’, the amplification of bias, and the erosion of trust
    in information by blurring the line between authentic and synthetic content. These dynamics
    threaten the integrity of the information ecosystem and complicate efforts to ensure
    accountability, transparency, and respect for human rights in the digital sphere.`,ie,R,ke=`At the root of the problem is a profit model that is based on the mass collection of data about
    our digital activities and behaviors. This is what Harvard scholar Shoshanna Zuboff called
    “surveillance capitalism” as early as 2014, before many of these dynamics escalated.<a class="fgoto" id="b3" href="#f3">[3]</a>
    Many platforms derive their revenue from targeted advertising, which is powered by opaque algorithmic
    systems designed to capture as much of our data as possible, and then use it to drive reach and engagement,
    and maximize corporate profits. This system creates the distribution channels through which divisive,
    discriminatory, hateful, and illegal online content flows. Big Tech platforms have concentrated gatekeeping
    power over information, not only by deciding what content is available, but by determining what content
    gets attention – by dominating how information is produced, prioritized, disseminated and monetized.
    In short, they control both the infrastructure and technology required to take part in the public
    debate, and hence over the information ecosystem itself. This raises not only technical and regulatory
    complexities, but also profound challenges to the democratic role of the media and the right to seek,
    receive, and impart information.`,re,I,le,B,_e="AI and human rights: a global public policy challenge",ce,W,Ae=`Many governments around the world are working to develop AI policies and regulations, as the
    pace of tech development and AI applications accelerates. In 2026, lawmakers in more than 70
    countries have launched efforts to develop domestic AI policies, amid ongoing initiatives aimed
    at coordinating multinational AI governance frameworks.<a class="fgoto" id="b4" href="#f4">[4]</a>`,de,O,Ie=`This has sparked wider debates among domestic and international policy makers, business leaders,
    and civil society over how to design AI policies that foster tech innovation while also
    safeguarding human rights. Policymakers must work urgently, and together with a wide spectrum of
    stakeholders, to establish adequate human rights protections and governance mechanisms to ensure
    that AI works to benefit society and to advance, rather than erode human rights and fundamental
    freedoms. This is particularly pressing in the area of media freedom and freedom of expression
    that are essential to the functioning of healthy democratic systems and societies. Technology
    should be leveraged to reinforce democratic processes and provide healthy information spaces for
    a well-informed public debate. Policymakers in many jurisdictions are turning to AI as a
    one-size-fits-all solution for tackling harmful or unlawful content online, despite the fact
    that these technologies are unreliable and pose serious risks to freedom of expression and the
    free flow of information and ideas online.`,he,G,Me=`In ongoing discussions about the use of AI to govern online content, some governments and
    policymakers have prioritized national security concerns over the protection of human
    rights—which rights groups and experts warn sets up a false dichotomy between protecting state
    security and safeguarding human rights. Lawmakers should therefore take care to avoid pitting
    security concerns against freedom of expression rights—as lasting, comprehensive security cannot
    be achieved without respect for human rights and functioning democratic institutions.`,ue,M,me,T,Te=`[1] For more details on the use of AI for content moderation and curation, see the 2022 Policy
    Manual “Spotlight on Artificial Intelligence and Freedom of Expression”,
    <a href="https://rfom.osce.org/representative-on-freedom-of-media/510332" target="_blank" rel="noopener noreferrer">https://rfom.osce.org/representative-on-freedom-of-media/510332</a> <a href="#b1" class="freturn">↩</a>`,fe,H,He=`[2] See “The truth behind filter bubbles: Bursting some myths.” Richard Fletcher. Reuters
    Institute for the Study of Journalism, University of Oxford, January 2020,
    <a href="https://reutersinstitute.politics.ox.ac.uk/news/truth-behind-filter-bubbles-bursting-some-myths" target="_blank" rel="noopener noreferrer">https://reutersinstitute.politics.ox.ac.uk/news/truth-behind-filter-bubbles-bursting-some-myths</a>. And “Beyond the Filter Bubble: Challenges for the Traditional Media and the Public
    Discourse,” in Are Algorithms a Threat to Democracy? The Rise of Intermediaries: A Challenge for
    Public Discourse. Birgit Stark, Daniel Stegmann, Melanie Magin, and Pascal Jürgens. Algorithm
    Watch, May 2020.
    <a href="https://algorithmwatch.org/en/wp-content/uploads/2020/05/Governing-Platforms-communications-study-Stark-May-2020-AlgorithmWatch.pdf" target="_blank" rel="noopener noreferrer">https://algorithmwatch.org/en/wp-content/uploads/2020/05/Governing-Platforms-communications-study-Stark-May-2020-AlgorithmWatch.pdf</a> <a href="#b2" class="freturn">↩</a>`,pe,P,Pe=`[3] Zuboff, Shoshana. 2019. The Age of Surveillance Capitalism: The Fight for a Human Future at
    the New Frontier of Power. First edition. New York: PublicAffairs. <a href="#b3" class="freturn">↩</a>`,ge,L,Le=`[4] OECD.AI Policy Navigator (2026), powered by EC/OECD (2026), database of national AI
    policies, accessed on 19/02/2026, https://oecd.ai. <a href="#b4" class="freturn">↩</a>`,Q;return n=new Be({props:{title:`Freedom of expression in the age of artificial intelligence:
  the risks and challenges to online speech and media freedom`,$$slots:{icon:[Ge]},$$scope:{ctx:A}}}),l=new ve({props:{class:"my-12",$$slots:{default:[Ne]},$$scope:{ctx:A}}}),I=new ve({props:{class:"my-12",$$slots:{default:[Je]},$$scope:{ctx:A}}}),M=new ve({props:{class:"my-12",$$slots:{default:[Ve]},$$scope:{ctx:A}}}),{c(){J(n.$$.fragment),x=u(),e=d("main"),i=d("p"),i.textContent=C,w=u(),r=d("p"),r.textContent=k,y=u(),c=d("p"),c.textContent=_,a=u(),J(l.$$.fragment),N=u(),p=d("h2"),p.textContent=$,ee=u(),j=d("p"),j.textContent=be,te=u(),F=d("p"),F.textContent=xe,ne=u(),q=d("p"),q.textContent=we,ae=u(),z=d("p"),z.textContent=ye,oe=u(),D=d("p"),D.innerHTML=$e,se=u(),E=d("p"),E.textContent=Ce,ie=u(),R=d("p"),R.innerHTML=ke,re=u(),J(I.$$.fragment),le=u(),B=d("h2"),B.textContent=_e,ce=u(),W=d("p"),W.innerHTML=Ae,de=u(),O=d("p"),O.textContent=Ie,he=u(),G=d("p"),G.textContent=Me,ue=u(),J(M.$$.fragment),me=u(),T=d("p"),T.innerHTML=Te,fe=u(),H=d("p"),H.innerHTML=He,pe=u(),P=d("p"),P.innerHTML=Pe,ge=u(),L=d("p"),L.innerHTML=Le,this.h()},l(v){V(n.$$.fragment,v),x=m(v),e=h(v,"MAIN",{class:!0});var t=Re(e);i=h(t,"P",{class:!0,"data-svelte-h":!0}),f(i)!=="svelte-1o2u3a0"&&(i.textContent=C),w=m(t),r=h(t,"P",{class:!0,"data-svelte-h":!0}),f(r)!=="svelte-1631x84"&&(r.textContent=k),y=m(t),c=h(t,"P",{class:!0,"data-svelte-h":!0}),f(c)!=="svelte-16z7va9"&&(c.textContent=_),a=m(t),V(l.$$.fragment,t),N=m(t),p=h(t,"H2",{class:!0,"data-svelte-h":!0}),f(p)!=="svelte-1ney133"&&(p.textContent=$),ee=m(t),j=h(t,"P",{class:!0,"data-svelte-h":!0}),f(j)!=="svelte-1s287xk"&&(j.textContent=be),te=m(t),F=h(t,"P",{class:!0,"data-svelte-h":!0}),f(F)!=="svelte-uiuc78"&&(F.textContent=xe),ne=m(t),q=h(t,"P",{class:!0,"data-svelte-h":!0}),f(q)!=="svelte-1jgr63k"&&(q.textContent=we),ae=m(t),z=h(t,"P",{class:!0,"data-svelte-h":!0}),f(z)!=="svelte-7qplbf"&&(z.textContent=ye),oe=m(t),D=h(t,"P",{class:!0,"data-svelte-h":!0}),f(D)!=="svelte-2qcmb"&&(D.innerHTML=$e),se=m(t),E=h(t,"P",{class:!0,"data-svelte-h":!0}),f(E)!=="svelte-1j565kr"&&(E.textContent=Ce),ie=m(t),R=h(t,"P",{class:!0,"data-svelte-h":!0}),f(R)!=="svelte-1pyu3y3"&&(R.innerHTML=ke),re=m(t),V(I.$$.fragment,t),le=m(t),B=h(t,"H2",{class:!0,"data-svelte-h":!0}),f(B)!=="svelte-1dt00j2"&&(B.textContent=_e),ce=m(t),W=h(t,"P",{class:!0,"data-svelte-h":!0}),f(W)!=="svelte-uvoonx"&&(W.innerHTML=Ae),de=m(t),O=h(t,"P",{class:!0,"data-svelte-h":!0}),f(O)!=="svelte-uow57a"&&(O.textContent=Ie),he=m(t),G=h(t,"P",{class:!0,"data-svelte-h":!0}),f(G)!=="svelte-1sds042"&&(G.textContent=Me),ue=m(t),V(M.$$.fragment,t),me=m(t),T=h(t,"P",{class:!0,id:!0,"data-svelte-h":!0}),f(T)!=="svelte-1xlbhpj"&&(T.innerHTML=Te),fe=m(t),H=h(t,"P",{class:!0,id:!0,"data-svelte-h":!0}),f(H)!=="svelte-5wyvgt"&&(H.innerHTML=He),pe=m(t),P=h(t,"P",{class:!0,id:!0,"data-svelte-h":!0}),f(P)!=="svelte-iu7j0w"&&(P.innerHTML=Pe),ge=m(t),L=h(t,"P",{class:!0,id:!0,"data-svelte-h":!0}),f(L)!=="svelte-vzkuus"&&(L.innerHTML=Le),t.forEach(g),this.h()},h(){s(i,"class","mb-4 leading-6"),s(r,"class","mb-4 leading-6"),s(c,"class","mb-4 leading-6"),s(p,"class","font-extrabold text-3xl leading-9 my-12"),s(j,"class","mb-4 leading-6"),s(F,"class","mb-4 leading-6"),s(q,"class","mb-4 leading-6"),s(z,"class","mb-4 leading-6"),s(D,"class","mb-4 leading-6"),s(E,"class","mb-4 leading-6"),s(R,"class","mb-4 leading-6"),s(B,"class","font-extrabold text-3xl leading-9 my-12"),s(W,"class","mb-4"),s(O,"class","mb-4"),s(G,"class","mb-4"),s(T,"class","mb-2 text-sm"),s(T,"id","f1"),s(H,"class","mb-2 text-sm"),s(H,"id","f2"),s(P,"class","mb-2 text-sm"),s(P,"id","f3"),s(L,"class","mb-2 text-sm"),s(L,"id","f4"),s(e,"class","container mx-auto py-12 px-4")},m(v,t){Z(n,v,t),b(v,x,t),b(v,e,t),o(e,i),o(e,w),o(e,r),o(e,y),o(e,c),o(e,a),Z(l,e,null),o(e,N),o(e,p),o(e,ee),o(e,j),o(e,te),o(e,F),o(e,ne),o(e,q),o(e,ae),o(e,z),o(e,oe),o(e,D),o(e,se),o(e,E),o(e,ie),o(e,R),o(e,re),Z(I,e,null),o(e,le),o(e,B),o(e,ce),o(e,W),o(e,de),o(e,O),o(e,he),o(e,G),o(e,ue),Z(M,e,null),o(e,me),o(e,T),o(e,fe),o(e,H),o(e,pe),o(e,P),o(e,ge),o(e,L),Q=!0},p(v,[t]){const Se={};t&1&&(Se.$$scope={dirty:t,ctx:v}),n.$set(Se);const je={};t&1&&(je.$$scope={dirty:t,ctx:v}),l.$set(je);const Fe={};t&1&&(Fe.$$scope={dirty:t,ctx:v}),I.$set(Fe);const qe={};t&1&&(qe.$$scope={dirty:t,ctx:v}),M.$set(qe)},i(v){Q||(U(n.$$.fragment,v),U(l.$$.fragment,v),U(I.$$.fragment,v),U(M.$$.fragment,v),Q=!0)},o(v){Y(n.$$.fragment,v),Y(l.$$.fragment,v),Y(I.$$.fragment,v),Y(M.$$.fragment,v),Q=!1},d(v){v&&(g(x),g(e)),K(n,v),K(l),K(I),K(M)}}}class nt extends De{constructor(n){super(),Ee(this,n,null,Ze,ze,{})}}export{nt as component};
